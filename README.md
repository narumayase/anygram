# anygram API

anygram is an API built with FastAPI that allows sending and receiving messages through a Telegram bot, integrating automatic responses generated by an LLM (Large Language Model).

The API exposes two main endpoints:

- **`/telegram/send`**: Allows sending messages to any Telegram chat through the bot.
- **`/telegram/webhook`**: Receives messages sent to the bot from Telegram, forwards the received text to an LLM API, and automatically replies to the user with the generated response.

## Project Structure

```
anygram
├── app
│   ├── api.py              # Endpoints 
│   ├── models.py           # Pydantic models or simple entities
│   ├── services.py         # Integrations (Telegram, LLM)
│   ├── config.py           # Configuration (dotenv, etc.)
│   └── main.py             # Entry point
├── requirements.txt
└── README.md
```

## Setup

1. Create a virtual environment:
   ```
   python -m venv venv
   source venv/bin/activate  # On Windows use `venv\Scripts\activate`
   ```

2. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

3. Create a `.env` file:
   ```
   # Telegram configuration (required)
   TELEGRAM_TOKEN=your_api_token
   TELEGRAM_API_URL="https://api.telegram.org"

   # LLM configuration (required)   
   LLM_URL=http://localhost:8081/api/v1/chat/ask

   # server configuration (optional)
   HOST=127.0.0.1
   PORT=8000
   RELOAD=true
   ```

## Usage

To run the application:
```
uvicorn app.main:app --host $HOST --port $PORT
```

By default, the API will be available at `http://127.0.0.1:8000`.

## Endpoints

### GET /health

Checks the API status.

```json
{
   "message": "anygram API is working!",
   "status": "ok",
   "host": "localhost",
   "port": "8000"
}
```

### Send a Message

POST to `/telegram/send` sends a message through the Telegram bot using the following body:

```json
{
  "chat_id": "123456789",
  "text": "Your message here"
}
```

### Webhook (Receive and Reply to Messages)

The `/telegram/webhook` endpoint receives messages from Telegram and automatically replies using the integration with the LLM API.

**Webhook Example Configuration:**

1. Expose the local API using [ngrok](https://ngrok.com/):
   ```
   ngrok http 8000
   ```

2. Set the webhook in Telegram:
   ```
   curl -X POST "https://api.telegram.org/bot<TELEGRAM_TOKEN>/setWebhook?url=https://<NGROK_URL>/telegram/webhook"
   ```
   Replace `<TELEGRAM_TOKEN>` and `<NGROK_URL>` with the correct values.

**How it works:**  

When the bot receives a message, the text is sent to the LLM API example :`http://localhost:8081/api/v1/chat/ask`, which should reply with a JSON:

```json
{
  "response": "Response text"
}
```
The bot then forwards that response back to the user on Telegram.

## Environment Variables

- `TELEGRAM_TOKEN`: Telegram bot token (stored in `.env`).
- `TELEGRAM_API_URL`: Telegram url (stored in `.env`).
- `LLM_URL`: LLM url (stored in `.env`).
- `HOST`: API host (stored in `.env`).
- `PORT`: API port (stored in `.env`).

# Testing

- All tests:

```bash
pytest
```

- All test with coverage:

```bash
pytest --cov=app --cov-report=term-missing tests/
```

## BackLog

- [ ] Unit Tests.
